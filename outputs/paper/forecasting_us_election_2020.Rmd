---
title: "Joe Biden projected to win Popular Vote in 2020 US Election with 51% of Vote"
subtitle: "Data given with 4 percent margin of error"
author: "Alen Mitrovski, Xiaoyan Yang, Matthew Wankiewicz"
date: "November 2nd, 2020"
abstract: "It seems like everyone has been waiting for the 2020 election, almost as soon as Donald Trump won in 2016. In our report, we use a logistic regression model along with multilevel regression  with post-stratification in order to predict who will win the election. According to our model, we predict that Joe Biden will win the popular vote over Donald Trump, 51% to 49%. These results are promising for the people of the United States, as they may be getting the change in President they need."
output:
  bookdown::pdf_document2:
    citation_package: natbib
    includes:  
      in_header: my_header.tex
toc: FALSE
bibliography: references.bib
---

```{r setup, echo=F, message=F, warning=F}
#install.packages("tidyverse")
#install.packages("gtsummary")
#install.packages("statebins")
#install.packages("broom")
##install.packages("kableExtra")
library(tidyverse)
library(statebins)
library(broom)
library(kableExtra)
```

```{r files, echo=F, message=F, warning=F}
# |**Keywords:** Forecasting, US 2020 Election, Trump, Biden, multilevel regression with post-stratification;

# read in the polling data
polling <- readRDS("data/polling_data.rds")

# read in post-strat data 
post_strat <- readRDS("data/post_strat.rds")

# read in electoral colleges data
elec_college <- read_csv("data/electoral_colleges.csv")
elec_college$State <- tolower(elec_college$State)
```

**Keywords**: Forecasting, US 2020 Election, Trump, Biden, multilevel regression with post-stratification;

# Introduction

# Data

We have used R (@citeR), specifically Tidyverse (@citeTidyverse) for data analysis

Data is from ACS (@citeIPUMS) and from Voter Study Group (@citeVSG).

```{r data1, echo=F, message=F, warning=F, fig.cap="Demographics of Sample and Population"}

# set up proportions and plots for polling data (variables we focus on)
# we do this by grouping by variable, getting the number of observations
# and then finding the percentage and classifying if they are polling or 
# post-strat data

# we do this grouping for gender, race, education level, state, age and 
# whether the respondent is hispanic or not
gender <- polling %>% 
  group_by(sex) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "poll",
         group = "gender") %>% 
  rename(level = sex)

races <- polling %>% 
  group_by(races) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "poll",
         group = "races") %>% 
  rename(level = races)

education <- polling %>% 
  group_by(education_level) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "poll",
         group = "education") %>% 
  drop_na(education_level) %>% 
  rename(level = education_level)

age <- polling %>% 
  group_by(age_groups) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "poll",
         group = "age") %>% 
  rename(level = age_groups)

statesicp <- polling %>% 
  group_by(stateicp) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "poll",
         group = "state") %>% 
   rename(level = stateicp)

hispanic <- polling %>% 
  group_by(hispanic) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "poll",
         group = "hispanic") %>% 
   rename(level = hispanic)

# set up proportions for post-strat data

# follow the same steps as above

gender_post <- post_strat %>% 
  group_by(sex) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "post-strat",
         group = "gender") %>% 
  rename(level = sex)

races_post <- post_strat %>% 
  group_by(races) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "post-strat",
         group = "races") %>% 
  rename(level = races)

education_post <- post_strat %>% 
  group_by(education_level) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "post-strat",
         group = "education") %>% 
  rename(level = education_level)

age_post <- post_strat %>% 
  group_by(age_groups) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "post-strat",
         group = "age") %>% 
  rename(level = age_groups)

statesicp_post <- post_strat %>% 
  group_by(stateicp) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "post-strat",
         group = "state") %>%
  rename(level = stateicp)

hispanic_post <- post_strat %>% 
  group_by(hispanic) %>% 
  summarise(n = n()) %>% 
  mutate(pct = n/sum(n), type = "post-strat",
         group = "hispanic") %>% 
  rename(level = hispanic)

# combine all of our grouped data into one data set
# called variables

variables <- rbind(hispanic, hispanic_post,
             age, age_post, gender, gender_post, 
             races, races_post)

# finally plot our results, grouped by variable with each line representing
# the proportions for the polling and post-strat data

variables %>% ggplot(aes(as.factor(level), pct, group=as.factor(type), linetype = as.factor(type))) + 
  geom_line() + facet_grid(~group, scales = "free") + 
  theme(axis.text.x = element_text(angle=70, size = 6, hjust = 1)) +
  labs(x = "Category", y = "Proportion", linetype = "data set") + 
  scale_y_continuous(labels = scales::percent) +
  geom_hline(yintercept = 0.5, alpha = 0.35)
```

Figure \@ref(fig:data1) show us the voter demographics from the VSG data (@citeVSG) vs the ACS data (@citeIPUMS).

```{r data2, echo = F, message=F, warning=F, fig.cap="More Demographics of Sample and Population"}
# combine educationa and state into one data set, need to separate these
# because text overlaps and makes the data look cleaner

variables2 <- rbind(education, education_post,
                    statesicp, statesicp_post)

# plot our results, grouped by variable with each line representing
# the proportions for the polling and post-strat data

variables2 %>% ggplot(aes(as.factor(level), pct, group=as.factor(type), linetype = as.factor(type))) + 
  geom_line() + facet_wrap(~group, nrow = 2, scales = "free") + 
  theme(axis.text.x = element_text(angle=70, size = 6, hjust = 1)) +
  labs(x = "Category", y = "Proportion", linetype = "data set") +
  scale_y_continuous(labels = scales::percent)
```

Figure \@ref(fig:data2) shows us more of the voter demographics from the VSG data (@citeVSG) vs the ACS data (@citeIPUMS).

```{r polling props, echo=F, message=F, warning=F}
# group the proportion of respondents by whether they'll vote for
# Biden or Trump

polling_vote <- polling %>% 
  mutate(vote_biden_clean = ifelse(vote_biden == 1, "Joe Biden", "Donald Trump")) %>% 
  group_by(vote_biden_clean) %>% 
  summarise(n = n()) %>%
  mutate(Proportion = round(n/sum(n)*100))

# rename the columns to make it more presentable

polling_vote <- polling_vote %>% 
  rename("Candidate" = vote_biden_clean,
         "Number of Respondents" = n,
         "Proportion (%)" = Proportion)

# plot the table along with number of respondents and proportions

kable(polling_vote, 
      caption = "Who decided voters plan to support (Polling Data)")
```

Table \@ref(tab:polling props) shows the proportion of decided voters who plan to vote for Donald Trump or Joe Biden. The data used to create this table is from the Voter Study Group (@citeVSG). 

# Model

For our analysis, we plan to use multilevel regression with post-stratification. Multilevel regression with post-stratification (MRP) is a type of analysis where we fit a model using a smaller data set, in this case our polling data and then use the results of the model to apply it to a larger population.

The main steps for MRP are: Find the data set you want to use to create your model. For our scenario, we used the polling data from the Voter Study Group (@citeVSG). Next, you must create a model using your smaller sample. We used logistic regression and the data used was the polling data. Our equation takes the form of equation \@ref(eq:logit), as seen below. Once you have your model, you must apply it to your larger data set to give an idea of the population. For our report, we used the Census data from IPUMS (@citeIPUMS).

MRP is extremely useful not only because of how simple it is but also because it allows us to estimate preferences of a population using individual responses from surveys. Also, as Kennedy and Gelman discuss in their paper "Know your population and know your model: Using model-based regression and post-stratification to generalize findings beyond the observed sample", MRP works best when you have a population and variables of interest and want to apply these variables using two data sets with different characteristics (@kennedy_gelman). Also, in relation to surveys, since you only use one survey to create your model, you don't encounter issues with having to have multiple surveys for each region/state and lets under-sampled populations to be represented through post-stratification. 

MRP does encounter some weaknesses as well. Once again we can look to Kennedy and Gelman's paper where they say that MRP is dependent on how accurate the model is (@kennedy_gelman). If the generated model makes incorrect predictions or assumptions, your post-stratification will be incorrect and will hurt your results.

To predict whether or not a person plans to vote for Joe Biden or Donald Trump, we plan to build a logistic regression model using data from the Voter Study Group (@citeVSG) and then post-stratify it using Census Data (@citeIPUMS). Since logistic regression only works for binary response variables, we created a variable called `vote_biden` which returns a 1 is the respondent plans to vote for Joe Biden and a 0 if they plan to vote for Donald Trump.

The logistic regression model takes the form of:

\begin{equation} 
  log(\frac{\hat{p}}{1 - \hat{p}}) = \beta_{0} + \beta_{1}x_{sex} + 
\beta_{2}x_{agegroup} + \beta_{3}x_{race} + \beta_{4}x_{state} + \beta_{5}x_{income} + \beta_{6}x_{hispanic}
  (\#eq:logit)
\end{equation} 

Once we have our regression model, we will use the `predict` function in R (@citeR), to apply our model to the Census data (@citeIPUMS). We do this by grouping the Census data by the demographics we plan to analyze (sex, race, age, education level, hispanic or not, state), and then applying the model to each of those groups. After applying the model, we will receive probabilities that a person in that group will vote for Joe Biden. Once the predictions are complete, we can use them to find out who will win the popular vote or how many electoral colleges a candidate will win. We also can use a 95 percent confidence interval, which means that we are 95 percent certain that true value for the population (in this case popular vote percentage), is within the range we obtain. From this we can also conclude that our results will be accurate between +/- 4%. 

In equation \@ref(eq:logit), each $\beta$ represents a coefficient that the regression model will compute for us. As for our variables, we have chosen to use sex, age, race, income, state, and whether the respondent is hispanic. We decided to use the first 3 because they are generally strong predictors of which candidate a person would support, such as how some states tend to vote republican year after year while some states flip between democratic and republican almost every election. Next, we decided to choose income, because Joe Biden has made claims to increase taxes on the rich, which may influence their support for him. Lastly, we wanted to focus on whether the respondent was hispanic and if so, where they were from. This variable was important for our predictions because we know how poorly Donald Trump has spoken of hispanic people and we believe they could have a strong impact on the election.

The output of the logistic regression model will give us a probability of whether or not a voter plans to vote for Joe Biden or not. In order to find this probability, we take the sum of the right side of equation \@ref(eq:logit) and plug it into the equation below:

\begin{equation} 
  \frac{e^{sum}}{1+e^{sum}}
  (\#eq:prob)
\end{equation} 

Equation \@ref(eq:prob) is just a manipulation of equation \@ref(eq:logit), where $e$ is the exponential equation and $sum$ is the sum of the right side of equation \@ref(eq:logit). We see that as the sum of the right side increases, the probability that a person will vote for Joe Biden increases as well. 
We are running our regression model using the `glm()` function in R (@citeR). The decision to run this model over other models like linear regression was made by the fact that we were predicting a binary variable about a voter's decision. Since there are only two possible options our data will likely follow an S shape and a straight line equation will not be helpful to model this relationship. Another strength present for logistic regression is that when combined with post-stratification it allows us to take information from under-represented populations and it allows their views to be accounted for more greatly. For example, our polling data (@citeVSG), includes only 2 observations from Wyoming, but using multilevel regression with post-stratification, we can have that expanded to over 3000 people!

Our model does have some weaknesses, since the output must be binary, we cannot account for other candidates or a person deciding not to vote. This issue isn't too large because our main goal is to determine which of the two main candidates will be chosen by the people of America. Another weakness we do encounter with our model and multi level regression with post-stratification is that it has a strong dependence on the survey data. This is a weakness because if the survey has any gaps or there are any tweaks we need to make, it can change the course of results.

```{r model, echo = F, message=F, warning=F, fig.cap="Results of Regression Model"}
#run the regression model using glm.
# sex, age_groups, race, stateicp, education_level and hispanic are used to 
# predit whether a person will vote biden or not

model <- glm(vote_biden ~ sex + age_groups + races +
               stateicp + education_level + hispanic, 
             data = polling, family = binomial())

# save the coefficients along with the confidence intervals,
# will be useful for our figures

coefficients <- broom::tidy(model, conf.int = T)

# create a table with the results of our coefficients along with p-values and
# confidence intervals

knitr::kable(coefficients,
             caption = "Coefficients from the Model") %>% 
  kableExtra::kable_styling(latex_options = "scale_down")
```


Table \@ref(tab:model), shows the estimates for the coefficients that will fit into our logistic regression equation. These coefficients will fit into Equation \@ref(eq:logit), and were calculated using data from the Voter Study Group (@citeVSG). The table is made using `kable` from `knitr` (@citeKnitr) and is formatted using `kableExtra` (@citekableExtra)

```{r coefficients, echo=F, message=F, warning=F, fig.height=4, fig.cap="Coefficient Estimates"}
# plot our coefficients with error bars to show the upper and lower
# estimates for the coefficients

coefficients %>% ggplot(aes(estimate, term)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  labs(title = "Coefficient Estimates",
       x = "Estimate", y = "Coefficient") +
  theme(axis.text.y = element_text(size = 5))
```

Figure \@ref(fig:coefficients) shows us the coefficients that would fit into equation \@ref(eq:logit) using the polling data (@citeVSG). We also have error bars present, which show the upper and lower estimates for the coefficients. What we have to look out for in this scenario is that coefficients with negative values would mean that the person is more likely to vote for Donald Trump (with that characteristic) and positive values mean the person is more likely to vote for Joe Biden. Table \@ref(tab:model) shows a numerical view of Figure \@ref(fig:coefficients), along with p-values. 

Using the outputs of the logisitic regression model, we can get an equation that follows the form of \@ref(eq:logit), but with the $\beta$ values filled out. This equation is difficult to write out because of the many variables, but in short, if the person's characteristic fits in with a certain variable, it is used in the equation. Then the equation is summed up and the probability is found using equation \@ref(eq:prob).

```{r cooks, echo=F, message=F, warning=F, fig.cap="Cook's Distance Plot for Model"}
# create a plot for cook's distance for the model, numbering the 3
# highest distances
plot(model, which = 4, id.n = 3)
```

Figure \@ref(fig:cooks) shows the cooks distance for observations in our polling data set. Cook's distance is useful for checking if a model is working correctly because it tells us how far a point is from the predicted value of it, telling us which points negatively impact our model. In our scenario, the distance between values would be the true value using our `glm` model vs what the `predict.glm` function returned. Figure \@ref(fig:cooks) takes all observations of our polling data (@citeVSG), and calculates the Cook's distance for it, showing which points can hurt the results of our model. We find that out of the 5200 obsevations, there aren't too many points that deviate from what we predict. This makes sense for our model because in the real world, there are some people who will support Donald Trump or Joe Biden, even if they don't "fit" the usual voter demographic for the candidate. Since the number of observations with large Cook's Distances are low, we can conclude that our model is fairly strong. 

# Results

```{r prediction, echo=F, warning=F, message=F}
# get proportions by demographic, then find proportion of each cell relative to population
props <- post_strat %>% 
  group_by(stateicp, races, age_groups,
           sex, education_level, hispanic) %>% 
  summarise(n = n()) %>% 
  group_by(stateicp) %>% 
  mutate(prop = n/sum(n))

# generate predictions for each of our grouped cells
props$estimate <- predict.glm(model, newdata = props,
                          type = "response")

# create a new column with number of voters from each cell voting Biden
props <- props %>% mutate(num_voters = n*estimate)

# get errors for our prediction
errors <- predict.glm(model, newdata = props,
                      type = "response", se.fit = T)
lower <- errors$fit - errors$se.fit
upper <- errors$fit + errors$se.fit


# combine the proportions and the errors we generated
props_error <- cbind(props, lower, upper)
props_error <- props_error %>% 
  rename("lower" = ...11,
         "upper" = ...12)

# create a new column with number of voters from each cell voting Biden
props_error <- props_error %>% mutate(num_voters_lower = n*lower,
                                num_voters_upper = n*upper)

# create a new dataset which contains upper and lower estimates for proportions
# of votes for each state
voting_biden <- props_error %>% mutate(biden_predict_prop=estimate*prop,
                                       biden_predict_prop_lower = lower*prop,
                                       biden_predict_prop_upper = upper*prop) %>%
  group_by(stateicp) %>%
  summarise(biden_predict = sum(biden_predict_prop),
            biden_predict_lower = sum(biden_predict_prop_lower),
            biden_predict_upper = sum(biden_predict_prop_upper))
```



```{r stateplot, echo=F, message=F, warning=F, fig.cap="Proportion of each State Voting for Biden"}
# get state by state proportions for our polling data
polling_props <- polling %>% 
  group_by(stateicp, vote_biden) %>% 
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n)) 

# combine the polling data proportions with the post-stratification data
poll_post_biden <- inner_join(polling_props, voting_biden, by = "stateicp")
poll_post_biden <- poll_post_biden %>% 
  filter(vote_biden == 1)

# plot for predictions

poll_post_biden %>% 
  ggplot(aes(x = stateicp, y = biden_predict)) +
  geom_point(aes(color = "black")) +
  geom_line(aes(group = 1)) +
  scale_y_continuous(labels = scales::percent) +
  theme(axis.text.x = element_text(angle = 70, size = 6, hjust = 1)) +
  geom_ribbon(aes(group = 1, ymin=biden_predict_lower, ymax=biden_predict_upper), 
              alpha = 0.2, fill = "red") +
  geom_line(data = poll_post_biden, aes(x=stateicp, y=prop, group = 2), 
            linetype = "twodash") +
  geom_point(data = poll_post_biden, aes(x=stateicp, y=prop, colour = "red")) +
  geom_hline(aes(yintercept = 0.5), alpha = 0.5) +
  scale_color_discrete(name = "Data", labels = c("Population", "Sample")) +
  labs(title = "Proportion of People voting for Joe Biden by State",
       x = "State", y = "Percentage Voting Biden")
```


Figure \@ref(fig:stateplot) shows us the proportion of respondents voting for Joe Biden broken down by state, along with the predicted proportions using MRP. We have also included the errors for each predicted value as well. We can see that the predicted proportions are fairly close to what the polling data showed. But, we also have to acknowledge the errors present, we can see that many states' errors cross over the 50% line which could be pivotal for each candidate because of the winner take all nature of the electoral college. We can see that some states like Florida and Georgia have small errors but there are some states who have very large ones like North Dakota and Wyoming.

```{r statemap, echo=F, message=F, warning=F, fig.cap="Proportion of Voters from Each State Voting Joe Biden"}
# plot state map

voting_biden %>% 
  mutate(statename = str_to_title(stateicp)) %>% 
  ggplot(aes(fill = biden_predict, state = statename)) + 
  geom_statebins() + 
  scale_fill_gradient2(low = "#d12531", high = "#244999",
                       mid = "white", midpoint = mean(voting_biden$biden_predict)) +
  theme_classic() +
  theme(axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank()) +
  labs(fill = "Proportion Voting \nfor Biden",
       title = "Map of the USA and which states plan to support Joe Biden or Donald Trump")
```

Figure \@ref(fig:statemap) is an amazing view of all the states and which candidate they are leaning towards voting for using `statebins` (@citeStatebins). We see that states like Vermont, Conneticut and California are some of the more "blue" states, meaning they plan to vote for Joe Biden, while North Dakota, Arkansas and Idaho are the "red" states, planning to vote for Donald Trump. The states that are more white in colour can be the most important for the race when it comes to deciding an actual winner through the electoral college. We see that Florida, Louisiana and New Hampshire are some of the more undecided states, and a switch in these states, can influence the election greatly.

```{r colleges, echo=F, message=F, warning=F}
# merge our electoral college data with predictions from model
states_biden_prop <- voting_biden %>%
  rename("State" = stateicp)
states_college <- inner_join(states_biden_prop, elec_college, by = "State")

# create new variables finding the upper, lower and mean prediction for number of colleges
states_college <- states_college %>% 
  mutate(biden_colleges = case_when(
    State == "Nebraska" ~ round(biden_predict*Number_of_colleges),
    State == "Maine" ~ round(biden_predict*Number_of_colleges),
    State != "Maine" | State != "Nebraska" ~ ifelse(biden_predict >= 0.5, Number_of_colleges, 0)
  ),
  biden_colleges_lower = case_when(
    State == "Nebraska" ~ round(biden_predict_lower*Number_of_colleges),
    State == "Maine" ~ round(biden_predict_lower*Number_of_colleges),
    State != "Maine" | State != "Nebraska" ~ ifelse(biden_predict_lower >= 0.5, Number_of_colleges, 0)),
  biden_colleges_upper = case_when(
    State == "Nebraska" ~ round(biden_predict_upper*Number_of_colleges),
    State == "Maine" ~ round(biden_predict_upper*Number_of_colleges),
    State != "Maine" | State != "Nebraska" ~ ifelse(biden_predict_upper >= 0.5, Number_of_colleges, 0)))


# compile upper, lower and mean number of colleges won, plus proportion of popular vote 
biden_upper <- c(round(sum(states_college$biden_colleges_upper)), 
                 (round(100*sum(props_error$num_voters_upper)/sum(props_error$n),1)))
biden_mid <- c(round(sum(states_college$biden_colleges)), 
                 (round(100*sum(props_error$num_voters)/sum(props_error$n), 1)))
biden_lower <- c(round(sum(states_college$biden_colleges_lower)), 
                 (round(100*sum(props_error$num_voters_lower)/sum(props_error$n), 1)))

# create a dataframe with all three estimates and create a table displaying the estimates
estimates <- tibble("Lower Estimate" = biden_lower, "Mean Estimate" = biden_mid, "Upper Estimate" = biden_upper)
rownames(estimates) <- c("Number of Colleges", "Proportion of Vote (%)")
kable(estimates, caption = "Joe Biden Voting Result Estimates") %>% 
  kable_styling(latex_options = "scale_down")
```

Table \@ref(tab:colleges) shows the lower, mean and upper predictions for the results of the election for Joe Biden. We see that on the lower end, Biden can expect to get 46.1% of the popular vote while only getting 191 electoral colleges. We also see that our middle estimate says Biden will get 51% of the popular vote while still losing the election by getting only 260 colleges. Lastly, on the upper estimate for Biden's results, he can get 55.9% of the popular vote while getting 423 colleges! This truly shows how close this election is, as we can see in Figure \@ref(fig:stateplot), many states are hovering around the 50% mark, which shows the colleges can go either way. The number of electoral colleges by state were found on the Britannica Encylopedia's website (@colleges).

```{r votingscales, echo=F, message=F, warning=F, fig.cap="Proportion of Biden votes by state"}
# arrange our proportions for each state in descending order

stateprops_plot <- poll_post_biden %>% arrange(desc(biden_predict)) %>% 
  mutate(support = ifelse(biden_predict >= 0.5, "Biden", "Trump"))

# plot the results, with a line on 50% because of the electoral college

stateprops_plot %>% ggplot(aes(y = reorder(stateicp, biden_predict), x = biden_predict, 
                               color = support)) +
  geom_point(stat = "identity") +
  geom_errorbar(aes(xmin = biden_predict_lower, xmax = biden_predict_upper)) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red") +
  labs(title = "Estimated Proportion of State Voting for Joe Biden",
       x = "Percentage", y= "State") +
  scale_x_continuous(labels = scales::percent) +
  theme(axis.text.y = element_text(size = 5)) +
  scale_color_manual(values = c("Blue", "Red"))
```

Figure \@ref(fig:votingscales) is a different view of Figure \@ref(fig:stateplot), this time only focusing on the predictions for each state with the errors included. We see that many states have error bars overlapping the 50% line, showing that many states are a toss up, given the nature of the electoral college. 

# Discussion

Lastly, we can take a look at the state of Ohio. Ohio has been one of the best predictors for who will win the presidential election. Ballotpedia calls the state a "bellwether" because since 1900, Ohio has voted for the winning candidate 93.33% of the time (@ballotpedia). This is a remarkable number and as seen in \@ref(fig:statemap), Ohio appears to be more red than blue which may give us evidence that although Biden won the popular vote, Trump may scrape out a victory.

# Code

Code supporting this analysis can be found at: https://github.com/matthewwankiewicz/US_election_forecast


